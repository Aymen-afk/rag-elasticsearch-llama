# Twitter RAG Chatbot with LLaMA 3.1 and Elasticsearch

This project is a Retrieval-Augmented Generation (RAG) chatbot application built using the LLaMA 3.1 8B model via Ollama, Elasticsearch as a vector database, and an Express.js-based chat interface. It allows users to ask questions based on indexed Twitter posts, and returns relevant answers generated by the language model.

## Features

- Embedding and generation using `llama3.1:8b` via Ollama
- Vector storage and search using Elasticsearch
- Chat application developed with Express.js
- Data indexing from a Twitter posts dataset
- Example usage included in `example.webm`

---

## Installation & Setup

### 1. Install Ollama

Follow the instructions from the official website: [https://ollama.com/download](https://ollama.com/download)


### 2. Pull the LLaMA 3.1 8B Model

Once Ollama is installed, pull the model:

```bash
ollama pull llama3.1:8b
```

### 3. Install and Run Elasticsearch

Download and install Elasticsearch from [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)

Start Elasticsearch so it runs on `127.0.0.1:9200`:

```bash
./bin/elasticsearch
```

> Make sure Java is installed and the port 9200 is available.

### 4. Install Python Requirements

Create a virtual environment (optional but recommended):

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

Then install the required Python libraries:

```bash
pip install -r requirements.txt
```

### 5. Index the Twitter Dataset


```bash
python index_data.py
```

This will generate embeddings and store them in Elasticsearch.

### 6. Start the Chat Application

Run the Express.js server:

```bash
node app/server.mjs
```

The chatbot will now be accessible locally (depending on your setup, usually at `http://localhost:3000`).

---

## Example

You can watch an example of the chatbot in action in the file: `example.webm`

---
